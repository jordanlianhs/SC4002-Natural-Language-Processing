{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 Sequence Tagging: NER\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1.1 Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1.1.1 Download Dependency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting seqeval\n",
      "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from seqeval) (1.26.1)\n",
      "Collecting scikit-learn>=0.21.3 (from seqeval)\n",
      "  Obtaining dependency information for scikit-learn>=0.21.3 from https://files.pythonhosted.org/packages/db/0d/1f6d2cd52c886707b00ddb7ed2504cbf10903a60a7bebcd71f0f77d53505/scikit_learn-1.3.1-cp311-cp311-macosx_12_0_arm64.whl.metadata\n",
      "  Downloading scikit_learn-1.3.1-cp311-cp311-macosx_12_0_arm64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from scikit-learn>=0.21.3->seqeval) (1.11.3)\n",
      "Collecting joblib>=1.1.1 (from scikit-learn>=0.21.3->seqeval)\n",
      "  Obtaining dependency information for joblib>=1.1.1 from https://files.pythonhosted.org/packages/10/40/d551139c85db202f1f384ba8bcf96aca2f329440a844f924c8a0040b6d02/joblib-1.3.2-py3-none-any.whl.metadata\n",
      "  Downloading joblib-1.3.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=2.0.0 (from scikit-learn>=0.21.3->seqeval)\n",
      "  Obtaining dependency information for threadpoolctl>=2.0.0 from https://files.pythonhosted.org/packages/81/12/fd4dea011af9d69e1cad05c75f3f7202cdcbeac9b712eea58ca779a72865/threadpoolctl-3.2.0-py3-none-any.whl.metadata\n",
      "  Downloading threadpoolctl-3.2.0-py3-none-any.whl.metadata (10.0 kB)\n",
      "Downloading scikit_learn-1.3.1-cp311-cp311-macosx_12_0_arm64.whl (9.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.3.2-py3-none-any.whl (302 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.2/302.2 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.2.0-py3-none-any.whl (15 kB)\n",
      "Building wheels for collected packages: seqeval\n",
      "  Building wheel for seqeval (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16162 sha256=9cadc52c2cc9d08d85b02cf024bbd9210ce83e90309660207daee424946e5a32\n",
      "  Stored in directory: /Users/dingangoh/Library/Caches/pip/wheels/bc/92/f0/243288f899c2eacdfa8c5f9aede4c71a9bad0ee26a01dc5ead\n",
      "Successfully built seqeval\n",
      "Installing collected packages: threadpoolctl, joblib, scikit-learn, seqeval\n",
      "Successfully installed joblib-1.3.2 scikit-learn-1.3.1 seqeval-1.2.2 threadpoolctl-3.2.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install seqeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import tqdm\n",
    "from seqeval.metrics import f1_score\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer, seed_everything\n",
    "from pytorch_lightning.loggers.csv_logs import CSVLogger\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATH  = 'Data/eng.train'\n",
    "DEVELOPMENT_PATH = 'Data/eng.testa'\n",
    "TEST_PATH = 'Data/eng.testb'\n",
    "OUTPUT_DIR = 'Data/output'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1.1.2 Download the pretrained word2vec embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader\n",
    "\n",
    "#Download the embeddings \"word2vec-google-news-300\"\n",
    "glove_vectors = gensim.downloader.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1.1.3 Query the vector of any word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.07421875e-01, -2.01171875e-01,  1.23046875e-01,  2.11914062e-01,\n",
       "       -9.13085938e-02,  2.16796875e-01, -1.31835938e-01,  8.30078125e-02,\n",
       "        2.02148438e-01,  4.78515625e-02,  3.66210938e-02, -2.45361328e-02,\n",
       "        2.39257812e-02, -1.60156250e-01, -2.61230469e-02,  9.71679688e-02,\n",
       "       -6.34765625e-02,  1.84570312e-01,  1.70898438e-01, -1.63085938e-01,\n",
       "       -1.09375000e-01,  1.49414062e-01, -4.65393066e-04,  9.61914062e-02,\n",
       "        1.68945312e-01,  2.60925293e-03,  8.93554688e-02,  6.49414062e-02,\n",
       "        3.56445312e-02, -6.93359375e-02, -1.46484375e-01, -1.21093750e-01,\n",
       "       -2.27539062e-01,  2.45361328e-02, -1.24511719e-01, -3.18359375e-01,\n",
       "       -2.20703125e-01,  1.30859375e-01,  3.66210938e-02, -3.63769531e-02,\n",
       "       -1.13281250e-01,  1.95312500e-01,  9.76562500e-02,  1.26953125e-01,\n",
       "        6.59179688e-02,  6.93359375e-02,  1.02539062e-02,  1.75781250e-01,\n",
       "       -1.68945312e-01,  1.21307373e-03, -2.98828125e-01, -1.15234375e-01,\n",
       "        5.66406250e-02, -1.77734375e-01, -2.08984375e-01,  1.76757812e-01,\n",
       "        2.38037109e-02, -2.57812500e-01, -4.46777344e-02,  1.88476562e-01,\n",
       "        5.51757812e-02,  5.02929688e-02, -1.06933594e-01,  1.89453125e-01,\n",
       "       -1.16210938e-01,  8.49609375e-02, -1.71875000e-01,  2.45117188e-01,\n",
       "       -1.73828125e-01, -8.30078125e-03,  4.56542969e-02, -1.61132812e-02,\n",
       "        1.86523438e-01, -6.05468750e-02, -4.17480469e-02,  1.82617188e-01,\n",
       "        2.20703125e-01, -1.22558594e-01, -2.55126953e-02, -3.08593750e-01,\n",
       "        9.13085938e-02,  1.60156250e-01,  1.70898438e-01,  1.19628906e-01,\n",
       "        7.08007812e-02, -2.64892578e-02, -3.08837891e-02,  4.06250000e-01,\n",
       "       -1.01562500e-01,  5.71289062e-02, -7.26318359e-03, -9.17968750e-02,\n",
       "       -1.50390625e-01, -2.55859375e-01,  2.16796875e-01, -3.63769531e-02,\n",
       "        2.24609375e-01,  8.00781250e-02,  1.56250000e-01,  5.27343750e-02,\n",
       "        1.50390625e-01, -1.14746094e-01, -8.64257812e-02,  1.19140625e-01,\n",
       "       -7.17773438e-02,  2.73437500e-01, -1.64062500e-01,  7.29370117e-03,\n",
       "        4.21875000e-01, -1.12792969e-01, -1.35742188e-01, -1.31835938e-01,\n",
       "       -1.37695312e-01, -7.66601562e-02,  6.25000000e-02,  4.98046875e-02,\n",
       "       -1.91406250e-01, -6.03027344e-02,  2.27539062e-01,  5.88378906e-02,\n",
       "       -3.24218750e-01,  5.41992188e-02, -1.35742188e-01,  8.17871094e-03,\n",
       "       -5.24902344e-02, -1.74713135e-03, -9.81445312e-02, -2.86865234e-02,\n",
       "        3.61328125e-02,  2.15820312e-01,  5.98144531e-02, -3.08593750e-01,\n",
       "       -2.27539062e-01,  2.61718750e-01,  9.86328125e-02, -5.07812500e-02,\n",
       "        1.78222656e-02,  1.31835938e-01, -5.35156250e-01, -1.81640625e-01,\n",
       "        1.38671875e-01, -3.10546875e-01, -9.71679688e-02,  1.31835938e-01,\n",
       "       -1.16210938e-01,  7.03125000e-02,  2.85156250e-01,  3.51562500e-02,\n",
       "       -1.01562500e-01, -3.75976562e-02,  1.41601562e-01,  1.42578125e-01,\n",
       "       -5.68847656e-02,  2.65625000e-01, -2.09960938e-01,  9.64355469e-03,\n",
       "       -6.68945312e-02, -4.83398438e-02, -6.10351562e-02,  2.45117188e-01,\n",
       "       -9.66796875e-02,  1.78222656e-02, -1.27929688e-01, -4.78515625e-02,\n",
       "       -7.26318359e-03,  1.79687500e-01,  2.78320312e-02, -2.10937500e-01,\n",
       "       -1.43554688e-01, -1.27929688e-01,  1.73339844e-02, -3.60107422e-03,\n",
       "       -2.04101562e-01,  3.63159180e-03, -1.19628906e-01, -6.15234375e-02,\n",
       "        5.93261719e-02, -3.23486328e-03, -1.70898438e-01, -3.14941406e-02,\n",
       "       -8.88671875e-02, -2.89062500e-01,  3.44238281e-02, -1.87500000e-01,\n",
       "        2.94921875e-01,  1.58203125e-01, -1.19628906e-01,  7.61718750e-02,\n",
       "        6.39648438e-02, -4.68750000e-02, -6.83593750e-02,  1.21459961e-02,\n",
       "       -1.44531250e-01,  4.54101562e-02,  3.68652344e-02,  3.88671875e-01,\n",
       "        1.45507812e-01, -2.55859375e-01, -4.46777344e-02, -1.33789062e-01,\n",
       "       -1.38671875e-01,  6.59179688e-02,  1.37695312e-01,  1.14746094e-01,\n",
       "        2.03125000e-01, -4.78515625e-02,  1.80664062e-02, -8.54492188e-02,\n",
       "       -2.48046875e-01, -3.39843750e-01, -2.83203125e-02,  1.05468750e-01,\n",
       "       -2.14843750e-01, -8.74023438e-02,  7.12890625e-02,  1.87500000e-01,\n",
       "       -1.12304688e-01,  2.73437500e-01, -3.26171875e-01, -1.77734375e-01,\n",
       "       -4.24804688e-02, -2.69531250e-01,  6.64062500e-02, -6.88476562e-02,\n",
       "       -1.99218750e-01, -7.03125000e-02, -2.43164062e-01, -3.66210938e-02,\n",
       "       -7.37304688e-02, -1.77734375e-01,  9.17968750e-02, -1.25000000e-01,\n",
       "       -1.65039062e-01, -3.57421875e-01, -2.85156250e-01, -1.66992188e-01,\n",
       "        1.97265625e-01, -1.53320312e-01,  2.31933594e-02,  2.06054688e-01,\n",
       "        1.80664062e-01, -2.74658203e-02, -1.92382812e-01, -9.61914062e-02,\n",
       "       -1.06811523e-02, -4.73632812e-02,  6.54296875e-02, -1.25732422e-02,\n",
       "        1.78222656e-02, -8.00781250e-02, -2.59765625e-01,  9.37500000e-02,\n",
       "       -7.81250000e-02,  4.68750000e-02, -2.22167969e-02,  1.86767578e-02,\n",
       "        3.11279297e-02,  1.04980469e-02, -1.69921875e-01,  2.58789062e-02,\n",
       "       -3.41796875e-02, -1.44042969e-02, -5.46875000e-02, -8.78906250e-02,\n",
       "        1.96838379e-03,  2.23632812e-01, -1.36718750e-01,  1.75781250e-01,\n",
       "       -1.63085938e-01,  1.87500000e-01,  3.44238281e-02, -5.63964844e-02,\n",
       "       -2.27689743e-05,  4.27246094e-02,  5.81054688e-02, -1.07910156e-01,\n",
       "       -3.88183594e-02, -2.69531250e-01,  3.34472656e-02,  9.81445312e-02,\n",
       "        5.63964844e-02,  2.23632812e-01, -5.49316406e-02,  1.46484375e-01,\n",
       "        5.93261719e-02, -2.19726562e-01,  6.39648438e-02,  1.66015625e-02,\n",
       "        4.56542969e-02,  3.26171875e-01, -3.80859375e-01,  1.70898438e-01,\n",
       "        5.66406250e-02, -1.04492188e-01,  1.38671875e-01, -1.57226562e-01,\n",
       "        3.23486328e-03, -4.80957031e-02, -2.48046875e-01, -6.20117188e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Query the vecotr of any word by specifcying the word as the key\n",
    "#glove_vectors['beautiful']\n",
    "glove_vectors['computer']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1.1 Finding most similar words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a) student"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words most similar to 'student':\n",
      "students: 0.7294867038726807\n"
     ]
    }
   ],
   "source": [
    "word = \"student\"\n",
    "\n",
    "# Check if word is in vocab\n",
    "if word in glove_vectors.key_to_index:\n",
    "    \n",
    "    # Use most_similar to find word with the most similar cosines\n",
    "    # topn =1 only lists the most similar word\n",
    "    similar_words = glove_vectors.most_similar(word, topn=1)  \n",
    "    print(f\"Words most similar to '{word}':\")\n",
    "    \n",
    "    # Print the similarity scores\n",
    "    for similar_word, similarity_score in similar_words:\n",
    "        print(f\"{similar_word}: {similarity_score}\")\n",
    "else:\n",
    "    print(f\"'{word}' is not in the vocabulary.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) Apple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words most similar to 'Apple':\n",
      "Apple_AAPL: 0.7456986308097839\n"
     ]
    }
   ],
   "source": [
    "word = \"Apple\"\n",
    "\n",
    "# Check if word is in vocab\n",
    "if word in glove_vectors.key_to_index:\n",
    "    \n",
    "    # Use most_similar to find word with the most similar cosines\n",
    "    # topn=1 only lists the most similar word\n",
    "    similar_words = glove_vectors.most_similar(word, topn=1)  \n",
    "    print(f\"Words most similar to '{word}':\")\n",
    "    \n",
    "    # Print the similarity scores\n",
    "    for similar_word, similarity_score in similar_words:\n",
    "        print(f\"{similar_word}: {similarity_score}\")\n",
    "else:\n",
    "    print(f\"'{word}' is not in the vocabulary.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c) apple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words most similar to 'apple':\n",
      "apples: 0.720359742641449\n"
     ]
    }
   ],
   "source": [
    "word = \"apple\"\n",
    "\n",
    "# Check if word is in vocab\n",
    "if word in glove_vectors.key_to_index:\n",
    "    \n",
    "    # Use most_similar to find word with the most similar cosines\n",
    "    # topn =1 only lists the most similar word\n",
    "    similar_words = glove_vectors.most_similar(word, topn=1)  \n",
    "    print(f\"Words most similar to '{word}':\")\n",
    "    \n",
    "    # Print the similarity scores\n",
    "    for similar_word, similarity_score in similar_words:\n",
    "        print(f\"{similar_word}: {similarity_score}\")\n",
    "else:\n",
    "    print(f\"'{word}' is not in the vocabulary.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1.2 a) Size of training, development and test files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processing files and converting to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_csv(filepath,name):\n",
    "    OUTPUT_PATH = os.path.join(OUTPUT_DIR,name)\n",
    "    HEADERS = [\"sentence_number\",'word','tag']\n",
    "    sentence_number = 1\n",
    "    with open(OUTPUT_PATH,'w',newline='') as csvfile:\n",
    "                    csv_writer = csv.writer(csvfile)\n",
    "                    csv_writer.writerow(HEADERS)\n",
    "    data =[]\n",
    "    with open(filepath, 'r') as file:\n",
    "    # Read the file line by line\n",
    "        for line in file:\n",
    "            # Check for blank lines\n",
    "            if line.strip() =='':\n",
    "                sentence_number+=1\n",
    "                with open(OUTPUT_PATH,'a',newline='') as csvfile:\n",
    "                    csv_writer = csv.writer(csvfile)\n",
    "                    csv_writer.writerow([]) \n",
    "            else:\n",
    "                # Write sentence_number, word and its tag to csv\n",
    "                words = line.split()\n",
    "                data.append(sentence_number)\n",
    "                data.append(words[0])\n",
    "                data.append(words[-1])\n",
    "                with open(OUTPUT_PATH,'a',newline='') as csvfile:\n",
    "                    csv_writer = csv.writer(csvfile)\n",
    "                    csv_writer.writerow(data) \n",
    "                data = []\n",
    "\n",
    "#Converting all data to csv                 \n",
    "convert_to_csv(TRAIN_PATH,'train.csv')\n",
    "convert_to_csv(DEVELOPMENT_PATH,'development.csv')\n",
    "convert_to_csv(TEST_PATH,'test.csv')\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of train file: 14987\n",
      "number of words : 23623\n",
      "tags : ['B-LOC', 'B-MISC', 'B-ORG', 'I-LOC', 'I-MISC', 'I-ORG', 'I-PER', 'O']\n"
     ]
    }
   ],
   "source": [
    "# Reading train.csv using pandas\n",
    "train_df = pd.read_csv(os.path.join(OUTPUT_DIR,'train.csv'))\n",
    "\n",
    "# Printing size, number of words and tags\n",
    "print(f\"Size of train file: {train_df['sentence_number'].iloc[-1]}\")\n",
    "print(f\"number of words : {len(train_df['word'].unique())}\")\n",
    "print(f\"tags : {sorted(train_df['tag'].unique())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Development File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of train file: 3466\n",
      "number of words : 9966\n",
      "tags : ['B-MISC', 'I-LOC', 'I-MISC', 'I-ORG', 'I-PER', 'O']\n"
     ]
    }
   ],
   "source": [
    "# Reading development.csv using Pandas\n",
    "development_df = pd.read_csv(os.path.join(OUTPUT_DIR,'development.csv'))\n",
    "\n",
    "# Printing size, number of words and tags\n",
    "print(f\"Size of train file: {development_df['sentence_number'].iloc[-1]}\")\n",
    "print(f\"number of words : {len(development_df['word'].unique())}\")\n",
    "print(f\"tags : {sorted(development_df['tag'].unique())}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of train file: 3684\n",
      "number of words : 9489\n",
      "tags : ['B-LOC', 'B-MISC', 'B-ORG', 'I-LOC', 'I-MISC', 'I-ORG', 'I-PER', 'O']\n"
     ]
    }
   ],
   "source": [
    "# Reading test.csv using pandas\n",
    "test_df = pd.read_csv(os.path.join(OUTPUT_DIR,'test.csv'))\n",
    "\n",
    "\n",
    "# Printing size, number of words and tags\n",
    "print(f\"Size of train file: {test_df['sentence_number'].iloc[-1]}\")\n",
    "print(f\"number of words : {len(test_df['word'].unique())}\")\n",
    "print(f\"tags : {sorted(test_df['tag'].unique())}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Q1b) choose an example sentence from the training set of CoNLL2003 that has at least two named entities with more than one word. Explain how to form complete named entities from the label for each word, and list all the named entities in this sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q1b(df):\n",
    "    while True:\n",
    "        # Keeps track of number of entities\n",
    "        count =0\n",
    "        \n",
    "        # Generate a random sentence number\n",
    "        rand = random.randint(1,df['sentence_number'].iloc[-1]+1)\n",
    "\n",
    "        # Filter df by sentence_number\n",
    "        mask = df['sentence_number'] == rand\n",
    "        rand_df = df[mask].reset_index()\n",
    "\n",
    "        # Check is to ensure we do not double count consecutive word with same tag\n",
    "        check = None\n",
    "\n",
    "        # Check through dataframe and count number of entities\n",
    "        for x in range(len(rand_df['tag'])):\n",
    "             \n",
    "            if x < len(rand_df['tag'])-1:\n",
    "                # Check tag !='O', next tag is same as current tag \n",
    "                if rand_df['tag'].iloc[x]!='O' and rand_df['tag'].iloc[x+1] == rand_df['tag'].iloc[x]:\n",
    "\n",
    "                    # updating check if tags are the same \n",
    "                    if check == rand_df['tag'].iloc[x-1]:\n",
    "                        check = rand_df['tag'].iloc[x]\n",
    "\n",
    "                    # Incrementing count and updating check\n",
    "                    else:\n",
    "                        count+=1\n",
    "                        check = rand_df['tag'].iloc[x]\n",
    "                        \n",
    "                # Return once count >=2\n",
    "                if count >=2:\n",
    "                    return rand_df          \n",
    "                \n",
    "                \n",
    "\n",
    "\n",
    "q1b_df = q1b(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence: In Home Health Inc said on Thursday it will appeal to the U.S. Federal District Court in Minneapolis a decision by the Health Care Financing Administration ( HCFA ) that denied reimbursement of certain costs under Medicaid . \n",
      "tag: I-ORG I-ORG I-ORG I-ORG O O O O O O O O I-ORG I-ORG I-ORG I-ORG O I-LOC O O O O I-ORG I-ORG I-ORG I-ORG O I-ORG O O O O O O O O I-MISC O \n",
      "sentence with tag: In/I-ORG Home/I-ORG Health/I-ORG Inc/I-ORG said/O on/O Thursday/O it/O will/O appeal/O to/O the/O U.S./I-ORG Federal/I-ORG District/I-ORG Court/I-ORG in/O Minneapolis/I-LOC a/O decision/O by/O the/O Health/I-ORG Care/I-ORG Financing/I-ORG Administration/I-ORG (/O HCFA/I-ORG )/O that/O denied/O reimbursement/O of/O certain/O costs/O under/O Medicaid/I-MISC ./O \n"
     ]
    }
   ],
   "source": [
    "def process(df):\n",
    "    sentence = ''\n",
    "    tag = ''\n",
    "    sen_and_tag = ''\n",
    "    for x in range (len(df['word'])):\n",
    "        sentence += str(df['word'].iloc[x])\n",
    "        sentence+= ' '\n",
    "        tag += str(df['tag'].iloc[x])\n",
    "        tag+=' '\n",
    "        sen_and_tag +=str(df['word'].iloc[x])\n",
    "        sen_and_tag +='/'\n",
    "        sen_and_tag += str(df['tag'].iloc[x])\n",
    "        sen_and_tag += ' '\n",
    "    print(f\"sentence: {sentence}\")\n",
    "    print(f\"tag: {tag}\")\n",
    "    print(f\"sentence with tag: {sen_and_tag}\")\n",
    "\n",
    "process(q1b_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list of name entities: ['In Home Health Inc ', 'U.S. Federal District Court ', 'Minneapolis ', 'Health Care Financing Administration ', 'HCFA ', 'Medicaid ']\n",
      "list of name entities with tags {'In Home Health Inc ': 'I-ORG', 'U.S. Federal District Court ': 'I-ORG', 'Minneapolis ': 'I-LOC', 'Health Care Financing Administration ': 'I-ORG', 'HCFA ': 'I-ORG', 'Medicaid ': 'I-MISC'}\n"
     ]
    }
   ],
   "source": [
    "def list_named_entities(df):\n",
    "    entity =''\n",
    "    l =[]\n",
    "    t = []\n",
    "    for x in range(len(df['word'])):\n",
    "        if df['tag'].iloc[x]!='O':\n",
    "            entity+=str(df['word'].iloc[x])\n",
    "            entity+=' '\n",
    "        else:\n",
    "            if (entity!=''):\n",
    "                l.append(entity)\n",
    "                t.append(str(df['tag'].iloc[x-1]))\n",
    "                entity = ''\n",
    "    d = {k: v for k, v in zip(l,t)} \n",
    "    return l,d\n",
    "\n",
    "l,d= list_named_entities(q1b_df)\n",
    "print(f\"list of name entities: {l}\")\n",
    "print(f\"list of name entities with tags {d}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3) Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23623"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z=list(train_df['word'].unique())\n",
    "len(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# class Model(nn.Module):\n",
    "#     def __init__(self, embedding_dim, hidden_dim, num_labels, pretrained_embeddings):\n",
    "#         super(Model, self).__init__()\n",
    "#         self.embedding = nn.Embedding.from_pretrained(pretrained_embeddings, freeze=True)  # Freeze the embeddings\n",
    "#         self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "#         self.linear = nn.Linear(2 * hidden_dim, num_labels)  # Multiply by 2 for bidirectional LSTM\n",
    "#         self.softmax = nn.Softmax(dim=2)\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         x = x.long()\n",
    "#         x = self.embedding(x)\n",
    "#         lstm_out, _ = self.lstm(x)\n",
    "#         logits = self.linear(lstm_out)\n",
    "#         output = self.softmax(logits)\n",
    "        \n",
    "#         return output\n",
    "    \n",
    "#     # Define hyperparameters\n",
    "# ner_labels = ['B-LOC', 'I-LOC', 'B-MISC', 'I-MISC', 'B-ORG', 'I-ORG', 'I-PER', 'O']\n",
    "# embedding_dim = 300\n",
    "# hidden_dim = 64 # Change to see if there are any improvements\n",
    "# num_labels = len(ner_labels)\n",
    "# pretrained_embeddings = torch.tensor(glove_vectors.vectors)\n",
    "\n",
    "# # Initialize the NER model\n",
    "# model = Model(embedding_dim, hidden_dim, num_labels, pretrained_embeddings)\n",
    "# # # not updating the weights during training\n",
    "# # model.embedding.weight.requires_grad = False\n",
    "# # Define your loss function and optimizer\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "# loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VOCAB_SIZE = 23623\n",
    "# EMBEDDING_DIM = 300\n",
    "# VOCAB_SIZE = 23623\n",
    "# EMBEDDING_DIM = 300\n",
    "# from gensim.models import KeyedVectors\n",
    "\n",
    "# # Load the pre-trained GloVe vectors (e.g., 100-dimensional vectors)\n",
    "# glove_model = KeyedVectors.load_word2vec_format('glove.6B.100d.txt', binary=False)\n",
    "\n",
    "# glove_vectors\n",
    "\n",
    "# # Define your vocabulary and vocabulary size\n",
    "\n",
    "# vocab = list(train_df['word'].unique()) # Replace with your actual vocabulary\n",
    "# vocab_size = len(VOCAB_SIZE)\n",
    "\n",
    "# # Initialize an embedding matrix with zeros\n",
    "# embedding_matrix = np.zeros((vocab_size, 300))  # 100 is the dimension of GloVe vectors\n",
    "\n",
    "# # Fill the embedding matrix with GloVe vectors for words in your vocabulary\n",
    "# for word, i in word_index.items():\n",
    "#     if word in glove_model:\n",
    "#         embedding_matrix[i] = glove_model[word]\n",
    "\n",
    "# # Now, 'embedding_matrix' contains GloVe vectors f\n",
    "\n",
    "# # Load pre-trained word embeddings (e.g., Word2Vec, GloVe)\n",
    "# # Replace 'embedding_matrix' with your actual embedding matrix\n",
    "# # Ensure 'embedding_matrix' has shape (vocab_size, embedding_dim)\n",
    "# # Set trainable=False to freeze the embeddings during training\n",
    "# pretrained_embedding_layer = Embedding(input_dim=VOCAB_SIZE,\n",
    "#                                        output_dim=EMBEDDING_DIM,\n",
    "#                                        weights=[embedding_matrix],\n",
    "#                                        input_length=1,\n",
    "#                                        trainable=False)\n",
    "\n",
    "# # Define the LSTM model\n",
    "# model = Sequential()\n",
    "\n",
    "# # Add the pre-trained embedding layer\n",
    "# model.add(pretrained_embedding_layer)\n",
    "\n",
    "# # Add an LSTM layer to capture the context of each word\n",
    "# model.add(LSTM(units=100, return_sequences=False))\n",
    "\n",
    "# # Add a Dense layer with softmax activation for word classification\n",
    "# model.add(Dense(units=VOCAB_SIZE, activation='softmax')\n",
    "\n",
    "# # Compile the model\n",
    "# model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# # Print a summary of the model architecture\n",
    "# model.summary()\n",
    "\n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
